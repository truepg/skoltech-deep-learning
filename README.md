# Deep Learning by Victor Lempitsky

## Homework 1
- Differentiation
- Modules of neural networks
  - Linear transform layer 
  - SoftMax, LogSoftMax
  - Batch normalization
  - Dropout
  - Activations: Leaky ReLU, ELU, SoftPlus
  - Negative LogLikelihood criterion: numerically unstable and stable
  - Optimizers: SGD with momentum, Adam
- MNIST classification

## Homework 2
- Classification on Tiny ImageNet dataset
- Experiments with different backbones

## Homework 3
- UNet implementation
- DeepLab implementation

## Homework 4
POS-tagging (part-of-speech) with Transformers
- Encoder Layer
- Multi-Head Attention Layer
- Position-wise Feedforward Layer
- Training Pipeline

